#!/usr/bin/env python3
"""Alternative research tool using GPT-4o-search-preview + report generation for material.md."""
from __future__ import annotations

import argparse
import datetime as dt
import json
import os
from pathlib import Path
from typing import List, Dict, Any

from openai import OpenAI
from openai import OpenAIError

ARTICLES_ROOT = Path("articles")


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Research with GPT-4o-search-preview, then save output to articles/<n>/material.md."
    )
    parser.add_argument("query", help="Research topic or question to investigate.")
    parser.add_argument(
        "--search-model",
        default=os.environ.get("PRIMARY_SEARCH_MODEL", "gpt-4o-search-preview"),
        help="OpenAI search model to use (default: %(default)s).",
    )
    parser.add_argument(
        "--report-model",
        default=os.environ.get("PRIMARY_RESEARCH_MODEL", "gpt-5-mini"),
        help="Model for report generation (default: %(default)s).",
    )
    parser.add_argument(
        "--strategy-model",
        default=os.environ.get("PRIMARY_STRATEGY_MODEL", "gpt-5-mini"),
        help="Model for research strategy planning (default: %(default)s).",
    )
    parser.add_argument(
        "--fallback-model",
        default=os.environ.get("FALLBACK_RESEARCH_MODEL", "gpt-5-nano"),
        help="Fallback model when primary models fail (default: %(default)s).",
    )
    parser.add_argument(
        "--max-queries",
        type=int,
        default=12,
        help="Maximum number of search queries to perform (default: %(default)s).",
    )
    parser.add_argument(
        "--depth",
        choices=["basic", "detailed", "comprehensive"],
        default="comprehensive",
        help="Research depth level (default: %(default)s).",
    )
    return parser.parse_args()


def next_article_dir(root: Path) -> Path:
    existing: List[int] = []
    if root.exists():
        for child in root.iterdir():
            if child.is_dir() and child.name.isdigit():
                existing.append(int(child.name))
    next_id = (max(existing) + 1) if existing else 1
    target = root / str(next_id)
    target.mkdir(parents=True, exist_ok=False)
    return target


def try_model_with_fallback(client, model: str, fallback_model: str, messages: list, **kwargs) -> any:
    """ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨ã‚’è©¦è¡Œã—ã€å¤±æ•—æ™‚ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨"""
    try:
        print(f"[INFO] Attempting to use model: {model}")
        return client.chat.completions.create(model=model, messages=messages, **kwargs)
    except Exception as e:
        error_str = str(e).lower()
        # ã‚ˆã‚Šå¹…å¹…ã„ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’ãƒˆãƒªã‚¬ãƒ¼
        should_fallback = any([
            "model" in error_str,
            "not found" in error_str, 
            "unavailable" in error_str,
            "permission" in error_str,
            "quota" in error_str,  # ã‚¯ã‚©ãƒ¼ã‚¿è¶…é
            "insufficient_quota" in error_str,
            "rate_limit" in error_str,
            "429" in error_str  # HTTP 429 Too Many Requests
        ])
        
        if should_fallback:
            print(f"[WARNING] Model {model} failed: {e}")
            print(f"[INFO] Falling back to model: {fallback_model}")
            try:
                return client.chat.completions.create(model=fallback_model, messages=messages, **kwargs)
            except Exception as fallback_e:
                print(f"[ERROR] Fallback model {fallback_model} also failed: {fallback_e}")
                raise fallback_e
        else:
            raise e


def get_next_section_strategy(client: OpenAI, topic: str, current_report: str, strategy_model: str, fallback_model: str, section_count: int) -> Dict[str, Any]:
    """ç¾åœ¨ã®ãƒ¬ãƒãƒ¼ãƒˆå†…å®¹ã‚’è¸ã¾ãˆã¦ã€æ¬¡ã«èª¿æŸ»ã™ã¹ãã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æˆ¦ç•¥çš„ã«æ±ºå®š"""
    
    system_prompt = f"""ã‚ãªãŸã¯æˆ¦ç•¥ã‚³ãƒ³ã‚µãƒ«ã‚¿ãƒ³ãƒˆã§ã™ã€‚ã“ã‚Œã¾ã§ã«ä½œæˆã•ã‚ŒãŸãƒ¬ãƒãƒ¼ãƒˆå†…å®¹ã‚’åˆ†æã—ã€ã•ã‚‰ã«ä¾¡å€¤ã‚’é«˜ã‚ã‚‹ãŸã‚ã«æ¬¡ã«èª¿æŸ»ã™ã¹ãã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’1ã¤æˆ¦ç•¥çš„ã«æ±ºå®šã—ã¦ãã ã•ã„ã€‚

## åˆ†æè¦ä»¶ï¼š
1. ç¾åœ¨ã®ãƒ¬ãƒãƒ¼ãƒˆã®å¼·ã¿ãƒ»å¼±ã¿ã‚’è©•ä¾¡
2. èª­è€…ã«ã¨ã£ã¦æœ€ã‚‚ä¾¡å€¤ã®é«˜ã„æ¬¡ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç‰¹å®š
3. æ—¢å­˜å†…å®¹ã¨ã®é‡è¤‡ã‚’é¿ã‘ã€æ–°ã—ã„ä¾¡å€¤ã‚’è¿½åŠ 
4. è«–ç†çš„ãªæµã‚Œã¨ä¸€è²«æ€§ã‚’è€ƒæ…®

## å‡ºåŠ›å½¢å¼ï¼ˆJSONï¼‰ï¼š
{{
  "should_continue": true/false,
  "analysis": "ç¾åœ¨ã®ãƒ¬ãƒãƒ¼ãƒˆã®è©•ä¾¡ã¨æ¬¡ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®å¿…è¦æ€§ï¼ˆ150å­—ç¨‹åº¦ï¼‰",
  "next_section": {{
    "section_title": "æ¬¡ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³å",
    "description": "ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®ç›®çš„ã¨ä¾¡å€¤ï¼ˆ100-150å­—ï¼‰",
    "key_questions": ["èª¿æŸ»ã™ã¹ãå…·ä½“çš„ãªè³ªå•1", "è³ªå•2", "è³ªå•3"],
    "expected_insights": "æœŸå¾…ã•ã‚Œã‚‹ä¾¡å€¤ãƒ»æ´å¯Ÿï¼ˆ50-80å­—ï¼‰",
    "priority": "high/medium/low"
  }}
}}

## ç¶™ç¶šåˆ¤æ–­åŸºæº–ï¼š
- æ—¢ã«ååˆ†åŒ…æ‹¬çš„: should_continue = false
- ã¾ã é‡è¦ãªä¾¡å€¤ã‚’è¿½åŠ å¯èƒ½: should_continue = true
- ã‚»ã‚¯ã‚·ãƒ§ãƒ³æ•°ãŒ{section_count}ä»¥ä¸Š: ç¶™ç¶šã‚’æ…é‡ã«åˆ¤æ–­

ç¾åœ¨ã®ãƒ¬ãƒãƒ¼ãƒˆã‚’åˆ†æã—ã€æœ€é©ãªæ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã‚’æ±ºå®šã—ã¦ãã ã•ã„ã€‚"""

    max_retries = 3
    for attempt in range(max_retries):
        try:
            # ã‚ˆã‚Šæ˜ç¢ºãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§JSONã‚’å¼·åˆ¶
            enhanced_prompt = f"""{system_prompt}

## é‡è¦: å¿…ãšJSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„
ä»¥ä¸‹ã®å³å¯†ãªJSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚ä»–ã®æ–‡å­—ã¯ä¸€åˆ‡å«ã‚ãªã„ã§ãã ã•ã„ï¼š

{{
  "should_continue": true,
  "analysis": "ç¾åœ¨ã®ãƒ¬ãƒãƒ¼ãƒˆã®è©•ä¾¡ã¨æ¬¡ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®å¿…è¦æ€§",
  "next_section": {{
    "section_title": "æ¬¡ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³å",
    "description": "ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®ç›®çš„ã¨ä¾¡å€¤",
    "key_questions": ["è³ªå•1", "è³ªå•2", "è³ªå•3"],
    "expected_insights": "æœŸå¾…ã•ã‚Œã‚‹ä¾¡å€¤",
    "priority": "high"
  }}
}}"""

            response = try_model_with_fallback(
                client=client,
                model=strategy_model,
                fallback_model=fallback_model,
                messages=[
                    {"role": "system", "content": enhanced_prompt},
                    {"role": "user", "content": f"ãƒˆãƒ”ãƒƒã‚¯: {topic}\n\nç¾åœ¨ã®ãƒ¬ãƒãƒ¼ãƒˆå†…å®¹:\n{current_report}\n\nä¸Šè¨˜ã‚’åˆ†æã—ã¦ã€æ¬¡ã«è¿½åŠ ã™ã¹ãã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’JSONå½¢å¼ã§ææ¡ˆã—ã¦ãã ã•ã„ã€‚"}
                ],
                max_completion_tokens=3000,
                response_format={"type": "json_object"}
            )
            
            content = response.choices[0].message.content.strip()
            print(f"Raw JSON response (attempt {attempt + 1}): {content}")
            
            import json as json_lib
            strategy = json_lib.loads(content)
            
            # å¿…è¦ãªãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®å­˜åœ¨ç¢ºèª
            if not all(key in strategy for key in ["should_continue", "analysis"]):
                raise ValueError("Missing required fields in JSON response")
            
            return strategy
            
        except Exception as e:
            print(f"Strategy planning attempt {attempt + 1} failed: {e}")
            if attempt == max_retries - 1:
                # æœ€å¾Œã®è©¦è¡Œã‚‚å¤±æ•—ã—ãŸå ´åˆã¯çµ‚äº†
                print("All retry attempts failed. Stopping research.")
                return {
                    "should_continue": False,
                    "analysis": f"æˆ¦ç•¥ãƒ—ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°ãŒ{max_retries}å›å¤±æ•—ã—ãŸãŸã‚çµ‚äº†ã—ã¾ã™",
                    "next_section": None
                }
            print(f"Retrying... ({attempt + 1}/{max_retries})")
            
    # ã“ã“ã«ã¯åˆ°é”ã—ãªã„ã¯ãšã§ã™ãŒã€å®‰å…¨ã®ãŸã‚
    return {
        "should_continue": False,
        "analysis": "äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ",
        "next_section": None
    }


def analyze_topic_and_create_research_plan(client: OpenAI, topic: str, strategy_model: str, max_sections: int = 8) -> Dict[str, Any]:
    """GPT-5-miniã§ãƒˆãƒ”ãƒƒã‚¯ã‚’åˆ†æã—ã€åŒ…æ‹¬çš„ãªèª¿æŸ»è¨ˆç”»ã‚’ä½œæˆ"""
    
    system_prompt = f"""ã‚ãªãŸã¯æˆ¦ç•¥ã‚³ãƒ³ã‚µãƒ«ã‚¿ãƒ³ãƒˆã§ã™ã€‚ä¸ãˆã‚‰ã‚ŒãŸãƒˆãƒ”ãƒƒã‚¯ã«ã¤ã„ã¦ã€æ¥µã‚ã¦åŒ…æ‹¬çš„ã§ä¾¡å€¤ã®é«˜ã„èª¿æŸ»ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã™ã‚‹ãŸã‚ã®æœ€é©ãªãƒªã‚µãƒ¼ãƒæˆ¦ç•¥ã‚’è¨­è¨ˆã—ã¦ãã ã•ã„ã€‚

## åˆ†æè¦ä»¶ï¼š
1. ãƒˆãƒ”ãƒƒã‚¯ã®æœ¬è³ªçš„ãªä¾¡å€¤ã¨é‡è¦æ€§ã‚’åˆ†æ
2. å¯¾è±¡èª­è€…ã®ãƒ‹ãƒ¼ã‚ºã¨çŸ¥ã‚ŠãŸã„æƒ…å ±ã‚’ç‰¹å®š
3. èª¿æŸ»ã™ã¹ãæ ¸å¿ƒçš„ãªé ˜åŸŸã‚’ä½“ç³»çš„ã«æ•´ç†
4. å„é ˜åŸŸã§å–å¾—ã™ã¹ãå…·ä½“çš„æƒ…å ±ã‚’æ˜ç¢ºåŒ–

## å‡ºåŠ›å½¢å¼ï¼š
ä»¥ä¸‹ã®JSONå½¢å¼ã§{max_sections}ã¤ã®ãƒªã‚µãƒ¼ãƒã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨­è¨ˆã—ã¦ãã ã•ã„ï¼š

{{
  "topic_analysis": "ãƒˆãƒ”ãƒƒã‚¯ã®é‡è¦æ€§ã¨ä¾¡å€¤ã®åˆ†æï¼ˆ200å­—ç¨‹åº¦ï¼‰",
  "target_audience": "æƒ³å®šèª­è€…ã¨ãã®ãƒ‹ãƒ¼ã‚ºï¼ˆ100å­—ç¨‹åº¦ï¼‰", 
  "research_sections": [
    {{
      "section_title": "ã‚»ã‚¯ã‚·ãƒ§ãƒ³å",
      "description": "ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®ç›®çš„ã¨èª¿æŸ»å†…å®¹ï¼ˆ100-150å­—ï¼‰",
      "key_questions": ["èª¿æŸ»ã™ã¹ãå…·ä½“çš„ãªè³ªå•1", "è³ªå•2", "è³ªå•3"],
      "expected_insights": "æœŸå¾…ã•ã‚Œã‚‹æ´å¯Ÿãƒ»ä¾¡å€¤ï¼ˆ50-80å­—ï¼‰"
    }}
  ]
}}

## å“è³ªåŸºæº–ï¼š
- å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯ç•°ãªã‚‹è¦–ç‚¹ãƒ»ä¾¡å€¤ã‚’æä¾›ã™ã‚‹
- å®Ÿç”¨çš„ã§ actionable ãªæƒ…å ±å–å¾—ã‚’é‡è¦–
- æœ€æ–°æ€§ã¨å…·ä½“æ€§ã®ãƒãƒ©ãƒ³ã‚¹ã‚’è€ƒæ…®
- å°‚é–€æ€§ã¨ä¸€èˆ¬ç†è§£æ€§ã‚’ä¸¡ç«‹"""

    max_retries = 3
    for attempt in range(max_retries):
        try:
            # JSONå½¢å¼ã‚’æ˜ç¢ºã«æŒ‡å®š
            enhanced_prompt = f"""{system_prompt}

## é‡è¦: å¿…ãšJSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„
ä»¥ä¸‹ã®å³å¯†ãªJSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ï¼š

{{
  "topic_analysis": "ãƒˆãƒ”ãƒƒã‚¯ã®é‡è¦æ€§ã¨ä¾¡å€¤ã®åˆ†æ",
  "target_audience": "æƒ³å®šèª­è€…ã¨ãã®ãƒ‹ãƒ¼ã‚º",
  "research_sections": [
    {{
      "section_title": "ã‚»ã‚¯ã‚·ãƒ§ãƒ³å",
      "description": "ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®ç›®çš„ã¨èª¿æŸ»å†…å®¹",
      "key_questions": ["è³ªå•1", "è³ªå•2", "è³ªå•3"],
      "expected_insights": "æœŸå¾…ã•ã‚Œã‚‹æ´å¯Ÿãƒ»ä¾¡å€¤"
    }}
  ]
}}"""

            response = try_model_with_fallback(
                client=client,
                model=strategy_model,
                fallback_model=fallback_model,
                messages=[
                    {"role": "system", "content": enhanced_prompt},
                    {"role": "user", "content": f"ä»¥ä¸‹ã®ãƒˆãƒ”ãƒƒã‚¯ã«ã¤ã„ã¦ã€åŒ…æ‹¬çš„ã§ä¾¡å€¤ã®é«˜ã„ãƒªã‚µãƒ¼ãƒæˆ¦ç•¥ã‚’è¨­è¨ˆã—ã¦ãã ã•ã„ï¼š\n\n{topic}"}
                ],
                max_completion_tokens=3000,
                response_format={"type": "json_object"}
            )
            
            content = response.choices[0].message.content.strip()
            print(f"Raw initial plan JSON: {content}")
            
            import json as json_lib
            plan = json_lib.loads(content)
            
            # å¿…è¦ãªãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®å­˜åœ¨ç¢ºèª
            if not all(key in plan for key in ["topic_analysis", "target_audience"]):
                raise ValueError("Missing required fields in initial plan JSON")
            
            return plan
            
        except Exception as e:
            print(f"Initial strategy planning attempt {attempt + 1} failed: {e}")
            if attempt == max_retries - 1:
                print("All retry attempts for initial planning failed. Using minimal plan.")
                return {
                    "topic_analysis": f"{topic}ã«ã¤ã„ã¦åŸºæœ¬çš„ãªèª¿æŸ»ã‚’å®Ÿæ–½ã—ã¾ã™ã€‚",
                    "target_audience": "ä¸€èˆ¬çš„ãªå­¦ç¿’è€…ãƒ»å®Ÿè·µè€…",
                    "research_sections": []
                }
            print(f"Retrying initial planning... ({attempt + 1}/{max_retries})")


def generate_search_queries(client: OpenAI, topic: str, search_model: str, max_queries: int = 12, depth: str = "comprehensive") -> List[str]:
    """ãƒˆãƒ”ãƒƒã‚¯ã‹ã‚‰åŒ…æ‹¬çš„ãªæ¤œç´¢ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆ"""
    
    # æ·±åº¦ã«å¿œã˜ã¦èª¿æŸ»ã‚«ãƒ†ã‚´ãƒªã‚’æ±ºå®š
    if depth == "basic":
        categories = ["åŸºæœ¬æƒ…å ±", "å®Ÿè·µæ–¹æ³•", "äº‹ä¾‹"]
        queries_per_category = max_queries // 3
    elif depth == "detailed":
        categories = ["åŸºæœ¬æƒ…å ±", "å®Ÿè·µæ–¹æ³•", "äº‹ä¾‹", "æœ€æ–°å‹•å‘", "èª²é¡Œãƒ»æ³¨æ„ç‚¹"]
        queries_per_category = max_queries // 5
    else:  # comprehensive
        categories = [
            "åŸºæœ¬æƒ…å ±ãƒ»å®šç¾©", "å®Ÿè·µçš„æ‰‹æ³•ãƒ»æˆ¦ç•¥", "æˆåŠŸäº‹ä¾‹ãƒ»ã‚±ãƒ¼ã‚¹ã‚¹ã‚¿ãƒ‡ã‚£", 
            "æœ€æ–°å‹•å‘ãƒ»å¸‚å ´åˆ†æ", "å¿…è¦ã‚¹ã‚­ãƒ«ãƒ»ãƒ„ãƒ¼ãƒ«", "åå…¥ãƒ»ä¾¡æ ¼è¨­å®š",
            "èª²é¡Œãƒ»ãƒªã‚¹ã‚¯ãƒ»æ³¨æ„ç‚¹", "å°‚é–€çŸ¥è­˜ãƒ»ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯"
        ]
        queries_per_category = max(1, max_queries // len(categories))
    
    system_prompt = f"""ã‚ãªãŸã¯æˆ¦ç•¥çš„ãƒªã‚µãƒ¼ãƒã®å°‚é–€å®¶ã§ã™ã€‚
ä¸ãˆã‚‰ã‚ŒãŸãƒˆãƒ”ãƒƒã‚¯ã«ã¤ã„ã¦ã€ä»¥ä¸‹ã®ã‚«ãƒ†ã‚´ãƒªã”ã¨ã«å…·ä½“çš„ã§åŠ¹æœçš„ãªæ¤œç´¢ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

ã‚«ãƒ†ã‚´ãƒª: {', '.join(categories)}

å„ã‚«ãƒ†ã‚´ãƒªã«ã¤ã{queries_per_category}å€‹ç¨‹åº¦ã®ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆã—ã€å…¨ä½“ã§{max_queries}å€‹ä»¥å†…ã«ã—ã¦ãã ã•ã„ã€‚

è¦ä»¶:
1. å„ã‚¯ã‚¨ãƒªã¯å…·ä½“çš„ã§å®Ÿç”¨çš„ãªæƒ…å ±ãŒå¾—ã‚‰ã‚Œã‚‹ã‚‚ã®ã«ã™ã‚‹
2. æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã§åŠ¹æœçš„ã«æ¤œç´¢ã§ãã‚‹è‡ªç„¶ãªæ—¥æœ¬èªã«ã™ã‚‹
3. ç•°ãªã‚‹è¦³ç‚¹ãƒ»æ·±åº¦ã§ã‚«ãƒãƒ¼ã™ã‚‹
4. æœ€æ–°ã®æƒ…å ±ã‚„å…·ä½“çš„ãªæ•°å€¤ãƒ»äº‹ä¾‹ãŒå«ã¾ã‚Œã‚‹ã‚ˆã†ãªã‚¯ã‚¨ãƒªã«ã™ã‚‹
5. å®Ÿè·µè€…ãŒæœ¬å½“ã«çŸ¥ã‚ŠãŸã„è©³ç´°ãªæƒ…å ±ã‚’å–å¾—ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹

å‡ºåŠ›å½¢å¼:
å„ã‚«ãƒ†ã‚´ãƒªã®è¦‹å‡ºã—ãªã—ã§ã€æ¤œç´¢ã‚¯ã‚¨ãƒªã®ã¿ã‚’1è¡Œãšã¤å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"""

    try:
        response = try_model_with_fallback(
            client=client,
            model=search_model,
            fallback_model=fallback_model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"ãƒˆãƒ”ãƒƒã‚¯: {topic}"}
            ],
            max_completion_tokens=800
        )
        
        queries_text = response.choices[0].message.content.strip()
        queries = [q.strip() for q in queries_text.split('\n') if q.strip()]
        
        # æœ€å¤§æ•°ã«åˆ¶é™
        return queries[:max_queries]
        
    except Exception as e:
        print(f"Search query generation failed: {e}")
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åŸºæœ¬çš„ãªã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆ
        return [
            topic,
            f"{topic} ã¨ã¯",
            f"{topic} æœ€æ–°æƒ…å ±"
        ][:max_queries]


def perform_section_research(client: OpenAI, section_info: Dict[str, Any], search_model: str) -> Dict[str, Any]:
    """ãƒªã‚µãƒ¼ãƒãƒ—ãƒ©ãƒ³ã®å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«å¯¾ã—ã¦å°‚é–€çš„ãªèª¿æŸ»ã‚’å®Ÿè¡Œ"""
    
    section_title = section_info["section_title"]
    description = section_info["description"]
    key_questions = section_info["key_questions"]
    
    # ã‚­ãƒ¼ã‚·ãƒ§ãƒ³ã‚’çµåˆã—ã¦åŒ…æ‹¬çš„ãªæ¤œç´¢ã‚¯ã‚¨ãƒªã‚’ä½œæˆ
    combined_query = f"{section_title}: {description}\nèª¿æŸ»é …ç›®: {', '.join(key_questions)}"
    
    system_prompt = f"""ã‚ãªãŸã¯å°‚é–€ãƒªã‚µãƒ¼ãƒãƒ£ãƒ¼ã§ã™ã€‚æŒ‡å®šã•ã‚ŒãŸã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«ã¤ã„ã¦ã€æ¤œç´¢æ©Ÿèƒ½ã‚’ä½¿ã£ã¦æ¥µã‚ã¦è©³ç´°ã§ä¾¡å€¤ã®é«˜ã„èª¿æŸ»ã‚’è¡Œã£ã¦ãã ã•ã„ã€‚

## ã‚»ã‚¯ã‚·ãƒ§ãƒ³æƒ…å ±ï¼š
- **ã‚¿ã‚¤ãƒˆãƒ«**: {section_title}
- **ç›®çš„**: {description}
- **èª¿æŸ»é …ç›®**: {', '.join(key_questions)}

## èª¿æŸ»è¦ä»¶ï¼š
1. **æœ€æ–°æƒ…å ±ã®é‡è¦–**: 2024-2025å¹´ã®æœ€æ–°ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’å„ªå…ˆ
2. **ãƒ‡ãƒ¼ã‚¿åé›†**: å…·ä½“çš„ãªæ•°å€¤ã€çµ±è¨ˆã€äº‹ä¾‹ã‚’è±Šå¯Œã«å«ã‚ã‚‹
3. **å°‚é–€æ€§**: æ¥­ç•Œã®å°‚é–€çŸ¥è­˜ã¨å®Ÿè·µçš„æ´å¯Ÿã‚’æä¾›
4. **å®Ÿç”¨æ€§**: èª­è€…ãŒå®Ÿéš›ã«æ´»ç”¨ã§ãã‚‹ actionable ãªæƒ…å ±
5. **ä¿¡é ¼æ€§**: ä¿¡é ¼ã§ãã‚‹ã‚½ãƒ¼ã‚¹ã‹ã‚‰ã®æƒ…å ±ã‚’é‡è¦–

## å‡ºåŠ›å½¢å¼ï¼š
- ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚¿ã‚¤ãƒˆãƒ«ã§å§‹ã‚ã‚‹
- ã‚µãƒ–ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’é©åˆ‡ã«æ§‹é€ åŒ–
- ç®‡æ¡æ›¸ãã‚„è¡¨ã‚’æ´»ç”¨ã—ã¦èª­ã¿ã‚„ã™ã
- é‡è¦ãªæƒ…å ±ã¯ãƒœãƒ¼ãƒ«ãƒ‰(**æ–‡å­—**)ã§å¼·èª¿
- å…·ä½“ä¾‹ã‚„æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã‚’è±Šå¯Œã«å«ã‚ã‚‹
- 3000-5000æ–‡å­—ç¨‹åº¦ã®è©³ç´°ãªå†…å®¹

æ¥µã‚ã¦ä¾¡å€¤ã®é«˜ã„å°‚é–€çš„ãªèª¿æŸ»ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚"""

    try:
        response = try_model_with_fallback(
            client=client,
            model=search_model,
            fallback_model=fallback_model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"ä»¥ä¸‹ã«ã¤ã„ã¦æ¥µã‚ã¦è©³ç´°ã«èª¿æŸ»ã—ã¦ãã ã•ã„:\n\n{combined_query}"}
            ],
            max_completion_tokens=8000
        )
        
        return {
            "section_title": section_title,
            "content": response.choices[0].message.content.strip(),
            "success": True,
            "model": search_model,
            "query_info": section_info
        }
        
    except Exception as e:
        print(f"Section research failed for '{section_title}': {e}")
        return {
            "section_title": section_title,
            "content": f"èª¿æŸ»ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}",
            "success": False,
            "error": str(e),
            "query_info": section_info
        }


def perform_search_research(client: OpenAI, query: str, search_model: str) -> Dict[str, Any]:
    """æ¤œç´¢æ©Ÿèƒ½ä»˜ããƒ¢ãƒ‡ãƒ«ã§èª¿æŸ»ã‚’å®Ÿè¡Œ"""
    system_prompt = """ã‚ãªãŸã¯å°‚é–€çš„ãªãƒªã‚µãƒ¼ãƒãƒ£ãƒ¼ã§ã™ã€‚ä¸ãˆã‚‰ã‚ŒãŸã‚¯ã‚¨ãƒªã«ã¤ã„ã¦ã€æ¤œç´¢æ©Ÿèƒ½ã‚’ä½¿ã£ã¦æœ€æ–°ã®æƒ…å ±ã‚’åé›†ã—ã€æ¥µã‚ã¦è©³ç´°ã§åŒ…æ‹¬çš„ãªèª¿æŸ»çµæœã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚

## å¿…é ˆèª¿æŸ»è¦³ç‚¹ï¼ˆå…¨ã¦è©³ç´°ã«èª¿ã¹ã‚‹ã“ã¨ï¼‰:
1. **åŸºæœ¬æƒ…å ±**: å®šç¾©ã€æ­´å²ã€èƒŒæ™¯ã€æ¦‚è¦
2. **è©³ç´°åˆ†æ**: ä»•çµ„ã¿ã€ãƒ—ãƒ­ã‚»ã‚¹ã€æŠ€è¡“çš„å´é¢
3. **æœ€æ–°å‹•å‘**: 2024å¹´ä»¥é™ã®æœ€æ–°æƒ…å ±ã€ãƒˆãƒ¬ãƒ³ãƒ‰ã€å¤‰åŒ–
4. **å…·ä½“ä¾‹ãƒ»äº‹ä¾‹**: æˆåŠŸäº‹ä¾‹ã€å¤±æ•—äº‹ä¾‹ã€ã‚±ãƒ¼ã‚¹ã‚¹ã‚¿ãƒ‡ã‚£
5. **å®Ÿè·µæ–¹æ³•**: å…·ä½“çš„ãªæ‰‹é †ã€ãƒ„ãƒ¼ãƒ«ã€ãƒªã‚½ãƒ¼ã‚¹
6. **æ•°å€¤ãƒ»ãƒ‡ãƒ¼ã‚¿**: çµ±è¨ˆã€å¸‚å ´è¦æ¨¡ã€åç›Šã€åŠ¹æœæ¸¬å®š
7. **å°‚é–€çŸ¥è­˜**: ä¸Šç´šè€…å‘ã‘æŠ€è¡“ã€ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ãªè¦‹è§£
8. **èª²é¡Œãƒ»ãƒªã‚¹ã‚¯**: å•é¡Œç‚¹ã€æ³¨æ„äº‹é …ã€å¯¾å‡¦æ³•
9. **å°†æ¥å±•æœ›**: äºˆæ¸¬ã€å±•æœ›ã€ç™ºå±•å¯èƒ½æ€§
10. **é–¢é€£æƒ…å ±**: é–¢é€£åˆ†é‡ã€ç«¶åˆã€ä»£æ›¿æ‰‹æ®µ

## èª¿æŸ»è¦ä»¶:
- å¿…ãšæœ€æ–°ã®æ¤œç´¢çµæœã‚’æ´»ç”¨ã™ã‚‹
- å„è¦³ç‚¹ã§1000æ–‡å­—ä»¥ä¸Šã®è©³ç´°ãªèª¬æ˜ã‚’æä¾›
- å…·ä½“çš„ãªæ•°å€¤ã€äº‹ä¾‹ã€å¼•ç”¨ã‚’å«ã‚ã‚‹
- å°‚é–€ç”¨èªã¯è©³ã—ãèª¬æ˜ã™ã‚‹
- å®Ÿè·µçš„ã§ actionable ãªæƒ…å ±ã‚’é‡è¦–ã™ã‚‹

æ¥µã‚ã¦è©³ç´°ã§ä¾¡å€¤ã®é«˜ã„èª¿æŸ»çµæœã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚"""

    try:
        response = try_model_with_fallback(
            client=client,
            model=search_model,
            fallback_model=fallback_model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"ä»¥ä¸‹ã«ã¤ã„ã¦æ¥µã‚ã¦è©³ç´°ã«èª¿æŸ»ã—ã¦ãã ã•ã„ï¼ˆä¸Šè¨˜ã®10ã®è¦³ç‚¹å…¨ã¦ã‚’ç¶²ç¾…ã—ã€å®Ÿç”¨çš„ã§æœ€æ–°ã®æƒ…å ±ã‚’æä¾›ã—ã¦ãã ã•ã„ï¼‰: {query}"}
            ],
            max_completion_tokens=8000
        )
        
        return {
            "query": query,
            "content": response.choices[0].message.content.strip(),
            "success": True,
            "model": search_model
        }
        
    except Exception as e:
        print(f"Search research failed for query '{query}': {e}")
        return {
            "query": query,
            "content": f"æ¤œç´¢èª¿æŸ»ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}",
            "success": False,
            "error": str(e)
        }


def add_section_to_report(client: OpenAI, topic: str, current_report: str, section_info: Dict[str, Any], section_content: str, report_model: str) -> str:
    """ç¾åœ¨ã®ãƒ¬ãƒãƒ¼ãƒˆã«æ–°ã—ã„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’çµ±åˆã—ã¦è¿½è¨˜"""
    
    system_prompt = f"""ã‚ãªãŸã¯å°‚é–€ãƒ¬ãƒãƒ¼ãƒˆãƒ©ã‚¤ã‚¿ãƒ¼ã§ã™ã€‚æ—¢å­˜ã®ãƒ¬ãƒãƒ¼ãƒˆã«æ–°ã—ã„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’è«–ç†çš„ã«çµ±åˆã—ã€ã‚ˆã‚Šä¾¡å€¤ã®é«˜ã„ãƒ¬ãƒãƒ¼ãƒˆã«ç™ºå±•ã•ã›ã¦ãã ã•ã„ã€‚

## ã‚¿ã‚¹ã‚¯ï¼š
1. æ–°ã—ã„ã‚»ã‚¯ã‚·ãƒ§ãƒ³å†…å®¹ã‚’æ—¢å­˜ãƒ¬ãƒãƒ¼ãƒˆã«é©åˆ‡ã«çµ±åˆ
2. è«–ç†çš„ãªæ§‹æˆã¨æµã‚Œã‚’ç¶­æŒ
3. é‡è¤‡ã‚’é¿ã‘ã€ç›¸ä¹—åŠ¹æœã‚’ç”Ÿã‚€å†…å®¹é…ç½®
4. å°‚é–€æ€§ã¨èª­ã¿ã‚„ã™ã•ã®ãƒãƒ©ãƒ³ã‚¹

## ã‚»ã‚¯ã‚·ãƒ§ãƒ³æƒ…å ±ï¼š
- **ã‚¿ã‚¤ãƒˆãƒ«**: {section_info.get('section_title', 'Unknown')}
- **ç›®çš„**: {section_info.get('description', 'N/A')}
- **æœŸå¾…ä¾¡å€¤**: {section_info.get('expected_insights', 'N/A')}

## å‡ºåŠ›è¦ä»¶ï¼š
- æ—¢å­˜ã®ãƒ¬ãƒãƒ¼ãƒˆæ§‹é€ ã‚’å°Šé‡
- æ–°ã—ã„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æœ€é©ãªä½ç½®ã«é…ç½®
- ã‚»ã‚¯ã‚·ãƒ§ãƒ³é–“ã®è«–ç†çš„ãªã¤ãªãŒã‚Šã‚’æ˜ç¢ºåŒ–
- ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼ã§ç¾ã—ãæ§‹é€ åŒ–
- å…¨ä½“ã®ä¸€è²«æ€§ã¨å°‚é–€æ€§ã‚’å‘ä¸Š

æ—¢å­˜ãƒ¬ãƒãƒ¼ãƒˆã‚’ç™ºå±•ã•ã›ã¦ã€ã‚ˆã‚ŠåŒ…æ‹¬çš„ã§ä¾¡å€¤ã®é«˜ã„å†…å®¹ã«ã—ã¦ãã ã•ã„ã€‚"""

    try:
        response = try_model_with_fallback(
            client=client,
            model=report_model,
            fallback_model=fallback_model,
            messages=[
                {"role": "system", "content": system_prompt},
                {
                    "role": "user", 
                    "content": f"æ—¢å­˜ãƒ¬ãƒãƒ¼ãƒˆ:\n\n{current_report}\n\n---\n\næ–°ã—ã„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ ({section_info.get('section_title', 'Unknown')}):\n\n{section_content}\n\n---\n\nä¸Šè¨˜ã‚’çµ±åˆã—ã¦ã€'{topic}'ã«ã¤ã„ã¦ã®ã‚ˆã‚ŠåŒ…æ‹¬çš„ãªãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚"
                }
            ],
            max_completion_tokens=20000
        )
        
        return response.choices[0].message.content.strip()
        
    except Exception as e:
        print(f"Section integration failed: {e}")
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å˜ç´”ã«è¿½è¨˜
        return f"{current_report}\n\n---\n\n## {section_info.get('section_title', 'New Section')}\n\n{section_content}"


def initialize_report(client: OpenAI, topic: str, research_plan: Dict[str, Any], report_model: str) -> str:
    """åˆæœŸãƒ¬ãƒãƒ¼ãƒˆï¼ˆãƒˆãƒ”ãƒƒã‚¯åˆ†æã¨ã‚¤ãƒ³ãƒˆãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ï¼‰ã‚’ç”Ÿæˆ"""
    
    topic_analysis = research_plan.get("topic_analysis", "")
    target_audience = research_plan.get("target_audience", "")
    
    system_prompt = f"""ã‚ãªãŸã¯å°‚é–€ãƒ¬ãƒãƒ¼ãƒˆãƒ©ã‚¤ã‚¿ãƒ¼ã§ã™ã€‚ãƒˆãƒ”ãƒƒã‚¯åˆ†æã«åŸºã¥ã„ã¦ã€é«˜å“è³ªãªãƒ¬ãƒãƒ¼ãƒˆã®å°å…¥éƒ¨åˆ†ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

## ãƒ¬ãƒãƒ¼ãƒˆè¦ä»¶ï¼š
1. **ã‚¿ã‚¤ãƒˆãƒ«ã¨ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼** (300-400å­—)
2. **ãƒˆãƒ”ãƒƒã‚¯åˆ†æã¨èƒŒæ™¯** (200-300å­—)
3. **å¯¾è±¡èª­è€…ã¨ãƒ¬ãƒãƒ¼ãƒˆã®ä¾¡å€¤** (150-200å­—)
4. **ãƒ¬ãƒãƒ¼ãƒˆæ§‹æˆã®æ¦‚è¦** (100-150å­—)

## å“è³ªåŸºæº–ï¼š
- å°‚é–€çš„ã‹ã¤åˆ†ã‹ã‚Šã‚„ã™ã„è¡¨ç¾
- èª­è€…ã®æœŸå¾…å€¤ã‚’é©åˆ‡ã«è¨­å®š
- å¾Œç¶šã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¸ã®è‡ªç„¶ãªå°å…¥
- ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼ã§ç¾ã—ãæ§‹é€ åŒ–

æ®µéšçš„ã«ç™ºå±•ã™ã‚‹é«˜å“è³ªãƒ¬ãƒãƒ¼ãƒˆã®åŸºç›¤ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚"""

    try:
        response = try_model_with_fallback(
            client=client,
            model=report_model,
            fallback_model=fallback_model,
            messages=[
                {"role": "system", "content": system_prompt},
                {
                    "role": "user", 
                    "content": f"ãƒˆãƒ”ãƒƒã‚¯: {topic}\n\nåˆ†æ: {topic_analysis}\n\nå¯¾è±¡èª­è€…: {target_audience}\n\nä¸Šè¨˜ã«åŸºã¥ã„ã¦ã€é«˜å“è³ªãƒ¬ãƒãƒ¼ãƒˆã®å°å…¥éƒ¨åˆ†ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚"
                }
            ],
            max_completion_tokens=3000
        )
        
        return response.choices[0].message.content.strip()
        
    except Exception as e:
        print(f"Report initialization failed: {e}")
        return f"# {topic}\n\n## æ¦‚è¦\n{topic_analysis}\n\n## å¯¾è±¡èª­è€…\n{target_audience}"


def generate_strategic_comprehensive_report(client: OpenAI, topic: str, research_plan: Dict[str, Any], section_results: List[Dict[str, Any]], report_model: str) -> str:
    """GPT-5-miniã§æˆ¦ç•¥çš„ã§åŒ…æ‹¬çš„ãªãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
    
    # æˆåŠŸã—ãŸèª¿æŸ»çµæœã®ã¿ã‚’ä½¿ç”¨
    successful_results = [r for r in section_results if r.get("success", False)]
    
    if not successful_results:
        return f"# {topic}\n\nèª¿æŸ»çµæœã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚æ‰‹å‹•ã§ã®æƒ…å ±åé›†ãŒå¿…è¦ã§ã™ã€‚"
    
    # ãƒˆãƒ”ãƒƒã‚¯åˆ†ææƒ…å ±ã‚’å–å¾—
    topic_analysis = research_plan.get("topic_analysis", "")
    target_audience = research_plan.get("target_audience", "")
    
    # ã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆ¥ã®èª¿æŸ»çµæœã‚’æ§‹é€ åŒ–
    sections_content = "\n\n---\n\n".join([
        f"## {result['section_title']}\n\n{result['content']}"
        for result in successful_results
    ])
    
    system_prompt = f"""ã‚ãªãŸã¯å°‚é–€ãƒ¬ãƒãƒ¼ãƒˆãƒ©ã‚¤ã‚¿ãƒ¼ã§ã™ã€‚ä»¥ä¸‹ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆ¥èª¿æŸ»çµæœã‚’çµ±åˆã—ã¦ã€æ¥µã‚ã¦é«˜å“è³ªã§å®Ÿç”¨çš„ãªåŒ…æ‹¬ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

## ãƒˆãƒ”ãƒƒã‚¯èƒŒæ™¯ï¼š
- **åˆ†æ**: {topic_analysis}
- **èª­è€…**: {target_audience}

## ãƒ¬ãƒãƒ¼ãƒˆè¦ä»¶ï¼š
1. **ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼**: é‡è¦ãƒã‚¤ãƒ³ãƒˆã®ç°¡æ½”ãªè¦ç´„ï¼ˆ300-400å­—ï¼‰
2. **ç›®æ¬¡**: å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®æ¦‚è¦
3. **è©³ç´°ã‚»ã‚¯ã‚·ãƒ§ãƒ³**: æä¾›ã•ã‚ŒãŸå„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’çµ±åˆãƒ»æœ€é©åŒ–
4. **å®Ÿè·µçš„ãªè¡Œå‹•æŒ‡é‡**: å…·ä½“çš„ãªãƒã‚¯ã‚¹ãƒˆã‚¹ãƒ†ãƒƒãƒ—ï¼ˆ300-500å­—ï¼‰
5. **ã¾ã¨ã‚ã¨å°†æ¥å±•æœ›**: ç·æ‹¬ã¨ä»Šå¾Œã®å±•æœ›ï¼ˆ200-300å­—ï¼‰

## å“è³ªåŸºæº–ï¼š
- æƒ…å ±ã®é‡è¤‡ã‚’é¿ã‘ã€ä¾¡å€¤ã‚ã‚‹å†…å®¹ã®ã¿ã‚’çµ±åˆ
- å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³é–“ã®è«–ç†çš„ãªã¤ãªãŒã‚Šã‚’æ˜ç¢ºã«ã™ã‚‹
- èª­è€…ãŒå®Ÿéš›ã«æ´»ç”¨ã§ãã‚‹ actionable ãªæƒ…å ±ã‚’é‡è¦–
- å°‚é–€æ€§ã‚’ä¿ã¡ãªãŒã‚‰åˆ†ã‹ã‚Šã‚„ã™ã„è¡¨ç¾ã‚’ä½¿ç”¨
- ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼ã§ç¾ã—ãæ§‹é€ åŒ–
- ç·æ–‡å­—æ•°15,000-20,000æ–‡å­—ã®è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ

æ¥µã‚ã¦ä¾¡å€¤ã®é«˜ã„ã€å°‚é–€çš„ã§å®Ÿç”¨çš„ãªãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚"""

    try:
        response = try_model_with_fallback(
            client=client,
            model=report_model,
            fallback_model=fallback_model,
            messages=[
                {"role": "system", "content": system_prompt},
                {
                    "role": "user", 
                    "content": f"ä»¥ä¸‹ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆ¥èª¿æŸ»çµæœã‚’çµ±åˆã—ã¦ã€'{topic}'ã«ã¤ã„ã¦ã®æœ€é«˜å“è³ªã®åŒ…æ‹¬ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„:\n\n{sections_content}"
                }
            ],
            max_completion_tokens=20000
        )
        
        return response.choices[0].message.content.strip()
        
    except Exception as e:
        print(f"Strategic report generation failed: {e}")
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆ¥çµæœã‚’ãã®ã¾ã¾çµ±åˆ
        fallback_report = f"# {topic}\n\n"
        fallback_report += f"## åˆ†ææ¦‚è¦\n{topic_analysis}\n\n" if topic_analysis else ""
        fallback_report += f"## å¯¾è±¡èª­è€…\n{target_audience}\n\n" if target_audience else ""
        fallback_report += sections_content
        fallback_report += f"\n\n*æ³¨: çµ±åˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {str(e)}*"
        return fallback_report


def generate_comprehensive_report(client: OpenAI, topic: str, research_results: List[Dict[str, Any]], report_model: str) -> str:
    """è¤‡æ•°ã®èª¿æŸ»çµæœã‹ã‚‰åŒ…æ‹¬çš„ãªãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
    
    # æˆåŠŸã—ãŸèª¿æŸ»çµæœã®ã¿ã‚’ä½¿ç”¨
    successful_results = [r for r in research_results if r.get("success", False)]
    
    if not successful_results:
        return f"# {topic}\n\nèª¿æŸ»çµæœã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚æ‰‹å‹•ã§ã®æƒ…å ±åé›†ãŒå¿…è¦ã§ã™ã€‚"
    
    # èª¿æŸ»çµæœã‚’ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦çµåˆ
    combined_research = "\n\n---\n\n".join([
        f"## èª¿æŸ»ã‚¯ã‚¨ãƒª: {result['query']}\n\n{result['content']}"
        for result in successful_results
    ])
    
    system_prompt = """ã‚ãªãŸã¯å°‚é–€çš„ãªãƒ¬ãƒãƒ¼ãƒˆãƒ©ã‚¤ã‚¿ãƒ¼ã§ã™ã€‚è¤‡æ•°ã®èª¿æŸ»çµæœã‚’çµ±åˆã—ã¦ã€æ¥µã‚ã¦åŒ…æ‹¬çš„ã§å®Ÿç”¨çš„ãªãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼ã®ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

## å¿…é ˆãƒ¬ãƒãƒ¼ãƒˆæ§‹æˆï¼ˆå…¨ã‚»ã‚¯ã‚·ãƒ§ãƒ³è©³ç´°åŒ–ï¼‰:
1. **ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼** (é‡è¦ãƒã‚¤ãƒ³ãƒˆã®è¦ç´„)
2. **åŸºæœ¬æƒ…å ±ã¨å®šç¾©** (è©³ç´°ãªæ¦‚è¦ã€æ­´å²çš„èƒŒæ™¯)
3. **æœ€æ–°å‹•å‘ã¨å¸‚å ´åˆ†æ** (2024å¹´ä»¥é™ã®ãƒˆãƒ¬ãƒ³ãƒ‰ã€çµ±è¨ˆãƒ‡ãƒ¼ã‚¿)
4. **è©³ç´°ãƒ¡ã‚½ãƒƒãƒ‰ã¨æŠ€è¡“è§£èª¬** (å…·ä½“çš„æ‰‹æ³•ã€ãƒ—ãƒ­ã‚»ã‚¹ã€æŠ€è¡“çš„å´é¢)
5. **å®Ÿè·µã‚¬ã‚¤ãƒ‰** (ã‚¹ãƒ†ãƒƒãƒ—ãƒã‚¤ã‚¹ãƒ†ãƒƒãƒ—ã®å®Ÿè¡Œæ–¹æ³•)
6. **æˆåŠŸäº‹ä¾‹ã¨ã‚±ãƒ¼ã‚¹ã‚¹ã‚¿ãƒ‡ã‚£** (å…·ä½“çš„ãªæˆåŠŸä¾‹ã€æ•°å€¤çµæœ)
7. **å¿…è¦ãªã‚¹ã‚­ãƒ«ã¨ãƒ„ãƒ¼ãƒ«** (è¦æ±‚ã•ã‚Œã‚‹èƒ½åŠ›ã€æ¨å¥¨ãƒ„ãƒ¼ãƒ«)
8. **åç›Šæ€§ã¨ä¾¡æ ¼è¨­å®š** (åå…¥ã®å¯èƒ½æ€§ã€å¸‚å ´ä¾¡æ ¼)
9. **èª²é¡Œã¨ãƒªã‚¹ã‚¯ç®¡ç†** (å•é¡Œç‚¹ã€å¯¾ç­–ã€æ³¨æ„äº‹é …)
10. **å°‚é–€çŸ¥è­˜ã¨ä¸Šç´šãƒ†ã‚¯ãƒ‹ãƒƒã‚¯** (ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«å‘ã‘æƒ…å ±)
11. **å°†æ¥å±•æœ›ã¨ç™ºå±•å¯èƒ½æ€§** (äºˆæ¸¬ã€æˆé•·æ€§)
12. **å®Ÿè·µçš„ãƒªã‚½ãƒ¼ã‚¹ã¨æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—** (å½¹ç«‹ã¤ãƒªãƒ³ã‚¯ã€è¡Œå‹•è¨ˆç”»)

## å“è³ªè¦ä»¶:
- å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³500-800æ–‡å­—ã®è©³ç´°ãªèª¬æ˜
- å…·ä½“çš„ãªæ•°å€¤ã€ãƒ‡ãƒ¼ã‚¿ã€äº‹ä¾‹ã‚’è±Šå¯Œã«å«ã‚ã‚‹
- å®Ÿè·µçš„ã§ actionable ãªæƒ…å ±ã‚’å„ªå…ˆ
- å°‚é–€ç”¨èªã«ã¯ä¸å¯§ãªè§£èª¬ã‚’ä»˜ä¸
- è«–ç†çš„ã§èª­ã¿ã‚„ã™ã„æ§‹é€ 
- é‡è¤‡ã‚’é¿ã‘ã€å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ç•°ãªã‚‹ä¾¡å€¤ã‚’æä¾›
- æ—¥æœ¬èªã§è‡ªç„¶ã‹ã¤ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ãªæ–‡ç« 
- ç·æ–‡å­—æ•°8000-12000æ–‡å­—ã®æ¥µã‚ã¦è©³ç´°ãªãƒ¬ãƒãƒ¼ãƒˆ

å¾“æ¥ã®3å€ä»¥ä¸Šã®æƒ…å ±é‡ã¨ä¾¡å€¤ã‚’æŒã¤ã€å°‚é–€æ€§ã®é«˜ã„ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚"""

    try:
        response = try_model_with_fallback(
            client=client,
            model=report_model,
            fallback_model=fallback_model,
            messages=[
                {"role": "system", "content": system_prompt},
                {
                    "role": "user", 
                    "content": f"ä»¥ä¸‹ã®èª¿æŸ»çµæœã‚’çµ±åˆã—ã¦ã€'{topic}'ã«ã¤ã„ã¦ã®åŒ…æ‹¬çš„ãªãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„:\n\n{combined_research}"
                }
            ],
            max_completion_tokens=16000
        )
        
        return response.choices[0].message.content.strip()
        
    except Exception as e:
        print(f"Report generation failed: {e}")
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: èª¿æŸ»çµæœã‚’ãã®ã¾ã¾æ§‹é€ åŒ–
        fallback_report = f"# {topic}\n\n"
        fallback_report += "## èª¿æŸ»çµæœ\n\n"
        fallback_report += combined_research
        fallback_report += f"\n\n*æ³¨: ãƒ¬ãƒãƒ¼ãƒˆçµ±åˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}*"
        return fallback_report


def main() -> int:
    args = parse_args()
    
    api_key = os.environ.get("OPENAI_API_KEY")
    if not api_key:
        print("Set the OPENAI_API_KEY environment variable first.", file=sys.stderr)
        return 1

    client = OpenAI(api_key=api_key)
    
    try:
        # æ–°ã—ã„è¨˜äº‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
        article_dir = next_article_dir(ARTICLES_ROOT)
        article_id = int(article_dir.name)
        
        print(f"[{dt.datetime.now().strftime('%H:%M:%S')}] Starting strategic iterative research for: {args.query}")
        print(f"[{dt.datetime.now().strftime('%H:%M:%S')}] Article directory: {article_dir}")
        print(f"[{dt.datetime.now().strftime('%H:%M:%S')}] ğŸ§  Using Iterative: GPT-5-mini strategy â†’ GPT-4o-search â†’ GPT-5-mini report â†’ repeat")
        
        # 1. åˆæœŸæˆ¦ç•¥ã¨ãƒ¬ãƒãƒ¼ãƒˆåŸºç›¤ã‚’ä½œæˆ
        print(f"\n[{dt.datetime.now().strftime('%H:%M:%S')}] ğŸ” Phase 1: Initial Research Planning & Report Foundation (GPT-5-mini)...")
        print(f"[{dt.datetime.now().strftime('%H:%M:%S')}] ğŸ“¡ OpenAI API REQUEST: GPT-5-mini ã§åˆæœŸæˆ¦ç•¥ãƒ—ãƒ©ãƒ³ç”Ÿæˆé–‹å§‹")
        
        initial_plan = analyze_topic_and_create_research_plan(client, args.query, args.strategy_model, max_sections=3)
        
        print(f"[{dt.datetime.now().strftime('%H:%M:%S')}] âœ… åˆæœŸæˆ¦ç•¥ãƒ—ãƒ©ãƒ³ç”Ÿæˆå®Œäº†")
        print(f"[{dt.datetime.now().strftime('%H:%M:%S')}] ğŸ“‹ Initial Strategy Created:")
        print(f"[{dt.datetime.now().strftime('%H:%M:%S')}]   Topic Analysis: {initial_plan.get('topic_analysis', 'N/A')[:100]}...")
        print(f"[{dt.datetime.now().strftime('%H:%M:%S')}]   Target Audience: {initial_plan.get('target_audience', 'N/A')}")
        
        # ãƒ¬ãƒãƒ¼ãƒˆåŸºç›¤ã‚’åˆæœŸåŒ–
        print(f"[{dt.datetime.now().strftime('%H:%M:%S')}] ğŸ“¡ OpenAI API REQUEST: GPT-5-mini ã§ãƒ¬ãƒãƒ¼ãƒˆåŸºç›¤ç”Ÿæˆé–‹å§‹")
        current_report = initialize_report(client, args.query, initial_plan, args.report_model)
        print(f"[{dt.datetime.now().strftime('%H:%M:%S')}] âœ… ãƒ¬ãƒãƒ¼ãƒˆåŸºç›¤ç”Ÿæˆå®Œäº†")
        print(f"[{dt.datetime.now().strftime('%H:%M:%S')}] ğŸ“„ Report foundation created: {len(current_report)} characters")
        
        # 2. æ®µéšçš„ãªã‚»ã‚¯ã‚·ãƒ§ãƒ³è¿½åŠ ãƒ«ãƒ¼ãƒ—
        print(f"\n[{dt.datetime.now().strftime('%H:%M:%S')}] ğŸ”„ Phase 2: Iterative Section Development...")
        section_count = 0
        max_iterations = 10
        all_sections = []
        
        for iteration in range(1, max_iterations + 1):
            print(f"\n[{dt.datetime.now().strftime('%H:%M:%S')}] === Iteration {iteration} ===")
            
            # 2a. æ¬¡ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³æˆ¦ç•¥ã‚’æ±ºå®š
            print(f"[{dt.datetime.now().strftime('%H:%M:%S')}] ğŸ” Strategy: Determining next section...")
            print(f"[{dt.datetime.now().strftime('%H:%M:%S')}] ğŸ“¡ OpenAI API REQUEST: GPT-5-mini ã§ã‚»ã‚¯ã‚·ãƒ§ãƒ³æˆ¦ç•¥åˆ†æé–‹å§‹")
            strategy = get_next_section_strategy(client, args.query, current_report, args.strategy_model, args.fallback_model, section_count)
            
            if not strategy.get("should_continue", False):
                print(f"[{dt.datetime.now().strftime('%H:%M:%S')}] âœ… Strategy decided to stop: {strategy.get('analysis', 'Complete')}")
                break
            
            next_section = strategy.get("next_section")
            if not next_section:
                print(f"[{dt.datetime.now().strftime('%H:%M:%S')}] âŒ No valid next section provided")
                break
                
            section_title = next_section.get("section_title", f"Section {iteration}")
            print(f"[{dt.datetime.now().strftime('%H:%M:%S')}] âœ… ã‚»ã‚¯ã‚·ãƒ§ãƒ³æˆ¦ç•¥æ±ºå®šå®Œäº†")
            print(f"[{dt.datetime.now().strftime('%H:%M:%S')}] ğŸ“‹ Next section: {section_title}")
            print(f"[{dt.datetime.now().strftime('%H:%M:%S')}]    Priority: {next_section.get('priority', 'unknown')}")
            print(f"[{dt.datetime.now().strftime('%H:%M:%S')}]    Description: {next_section.get('description', 'N/A')[:100]}...")
            
            # 2b. ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’èª¿æŸ»
            print(f"[{dt.datetime.now().strftime('%H:%M:%S')}] ğŸ” Research: {section_title}...")
            print(f"[{dt.datetime.now().strftime('%H:%M:%S')}] ğŸ“¡ OpenAI API REQUEST: GPT-4o-search-preview ã§è©³ç´°èª¿æŸ»é–‹å§‹")
            section_result = perform_section_research(client, next_section, args.search_model)
            
            if section_result["success"]:
                print(f"[{dt.datetime.now().strftime('%H:%M:%S')}] âœ… Research success: {len(section_result['content'])} characters")
                
                # 2c. ãƒ¬ãƒãƒ¼ãƒˆã«çµ±åˆ
                print(f"[{dt.datetime.now().strftime('%H:%M:%S')}] ğŸ“ Integration: Adding to report...")
                print(f"[{dt.datetime.now().strftime('%H:%M:%S')}] ğŸ“¡ OpenAI API REQUEST: GPT-5-mini ã§ãƒ¬ãƒãƒ¼ãƒˆçµ±åˆé–‹å§‹")
                current_report = add_section_to_report(
                    client, args.query, current_report, next_section, 
                    section_result["content"], args.report_model
                )
                print(f"[{dt.datetime.now().strftime('%H:%M:%S')}] âœ… Integration success: {len(current_report)} total characters")
                print(f"[{dt.datetime.now().strftime('%H:%M:%S')}] ğŸ“ˆ Progress: ã‚»ã‚¯ã‚·ãƒ§ãƒ³ {section_count + 1} å®Œäº†ï¼ˆç´¯ç© {len(current_report)} æ–‡å­—ï¼‰")
                
                section_count += 1
                all_sections.append({
                    "iteration": iteration,
                    "section_info": next_section,
                    "research_result": section_result,
                    "report_length": len(current_report)
                })
            else:
                print(f"[{dt.datetime.now().strftime('%H:%M:%S')}] âŒ Research failed: {section_result.get('error', 'Unknown error')}")
                print(f"[{dt.datetime.now().strftime('%H:%M:%S')}] ğŸ”„ Continuing to next iteration...")
                # å¤±æ•—ã—ãŸå ´åˆã¯ç¶šè¡Œ
        
        final_report = current_report
        print(f"\n[{dt.datetime.now().strftime('%H:%M:%S')}] ğŸ‰ Iterative research completed after {section_count} sections!")
        print(f"[{dt.datetime.now().strftime('%H:%M:%S')}] ğŸ“Š Final report length: {len(final_report)} characters")
        
        # 4. material.mdã«ä¿å­˜
        material_file = article_dir / "material.md"
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
        metadata = {
            "timestamp": dt.datetime.utcnow().isoformat() + "Z",
            "query": args.query,
            "strategy_model": args.strategy_model,
            "search_model": args.search_model,
            "report_model": args.report_model,
            "fallback_model": args.fallback_model,
            "initial_plan": initial_plan,
            "successful_sections": section_count,
            "total_iterations": len(all_sections),
            "section_details": all_sections,
            "architecture": "Iterative: GPT-5-mini strategy â†’ GPT-4o-search â†’ GPT-5-mini report â†’ repeat"
        }
        
        full_content = f"""# {args.query}

## Research Metadata
- Timestamp: {metadata["timestamp"]}
- Architecture: {metadata["architecture"]}
- Strategy Model: {args.strategy_model}
- Search Model: {args.search_model}
- Report Model: {args.report_model}
- Successful Sections: {metadata["successful_sections"]}
- Total Iterations: {metadata["total_iterations"]}
- Method: Iterative strategic research

---

{final_report}

---

## Research Process
Iterative Development:
{chr(10).join([f"- Iteration {section['iteration']}: {section['section_info'].get('section_title', 'Unknown')} ({section['report_length']} chars total)" for section in all_sections])}

Research metadata:
```json
{json.dumps(metadata, ensure_ascii=False, indent=2)}
```
"""
        
        material_file.write_text(full_content, encoding="utf-8")
        
        print(f"\n[{dt.datetime.now().strftime('%H:%M:%S')}] âœ… Iterative Strategic Research completed successfully!")
        print(f"[{dt.datetime.now().strftime('%H:%M:%S')}] ğŸ“ Article ID: {article_id}")
        print(f"[{dt.datetime.now().strftime('%H:%M:%S')}] ğŸ“ Material saved: {material_file}")
        print(f"[{dt.datetime.now().strftime('%H:%M:%S')}] ğŸ“Š Report length: {len(final_report)} characters")
        print(f"[{dt.datetime.now().strftime('%H:%M:%S')}] ğŸ” Successful sections: {metadata['successful_sections']}")
        print(f"[{dt.datetime.now().strftime('%H:%M:%S')}] ğŸ”„ Total iterations: {metadata['total_iterations']}")
        print(f"[{dt.datetime.now().strftime('%H:%M:%S')}] ğŸ§  Architecture: {metadata['architecture']}")
        print(f"[{dt.datetime.now().strftime('%H:%M:%S')}] ğŸ¯ Process Summary:")
        for section in all_sections:
            print(f"[{dt.datetime.now().strftime('%H:%M:%S')}]   - Section {section['iteration']}: {section['section_info'].get('section_title', 'Unknown')} â†’ {section['report_length']} chars")
        
        return 0
        
    except Exception as e:
        print(f"Research failed: {e}", file=sys.stderr)
        return 1


if __name__ == "__main__":
    import sys
    raise SystemExit(main())